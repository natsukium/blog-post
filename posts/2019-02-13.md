---
layout: post
title: Inductive Representation Learning on Large Graphs
author: natsukium
published: 2019-02-13
slug: journal-club
tags: deep, gnn
description: GraphSAGEの論文紹介
---

今回紹介する論文は次の論文です.
[Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216)

### 解決したい課題
大規模グラフにおいてノードを低次元ベクトルにembeddingする方法は数々提案されてきましたが, これらはどれもtransductiveで1つのグラフに対してのみ行われてきました.
このようなアプローチでは未知のノードを効果的にembeddingすることができず, 複数グラフにまたがったデータを学習できないという問題がありました.
そこでこれらを解決する手法としてGraphSAGEが提案されています.

## 概要
- GraphSAGEはGraph Convolution Networks(GCN)を拡張したもので, inductiveな問題設定に対応できる.

- GCNとは異なり, 周辺ノードの情報をaggregateする関数を学習する.

- ノードが追加されていくようなグラフや完全に新しいグラフであってもノードを分類することができる.

## Graph Neural Networks(GNN)のタスク
GraphSAGEの説明に入る前にGNNはどのようなものであるか整理しておきます.

GNNの主なタスクとして次のようなものがあります.
- Node Classification
  ノード分類. ノードの特徴量を予測する.
- Graph Classification
  グラフ分類. グラフ自体の性質を予測する.
- Link Prediction
  リンク予測. あるノード間にエッジが存在するかを予測する.
- Edge Classification
  エッジ分類. ノード間にどのような関係性があるか予測する.

さらにこれらは訓練データ, テストデータの使い方によって次の二つに分類されます.

- transductive (semi-supervised)
  訓練データとテストデータは同じグラフを用いる.
  グラフ中のラベルがついていないノードの予測をすることによってテストとする.
- inductive
  訓練データとテストデータは別のグラフを用いる.
  グラフ中の新規ノードや未知のグラフのノードを予測することによってテストとする.

今回紹介するGraphSAGEはNode Classificationかつinductiveな問題設定に該当します.

## アーキテクチャ
### 一目でわかるGraphSAGE
Fig.1を引用します.
![graphsage overview](https://raw.githubusercontent.com/natsukium/blog-post/master/images/2019-02-13/graphsage_overview.png)


### アルゴリズム
![graphsage algo](https://raw.githubusercontent.com/natsukium/blog-post/master/images/2019-02-13/graphsage_algo.png)

単純明快ですね.

### Neighborhood Function
$\mathcal{N}(v)$をノードの集合$\{ u \in \mathcal(V): (u, v) \in \mathcal{E} \}$から一定数ランダムにサンプリングする関数として定義します.
なお, イテレーション$k$ごとにサンプリングし直されることとします.

このサンプリングを行うことによって, 空間, 時間計算量を抑えることができ, 大規模グラフに対してもスケールすることができるようになります.
今回のケースでは以下の計算量に抑えられます.
$O(\prod^{K}_{i=1}S_{i}), K=2, S_{1} \cdot S_{2} \le 500$
　
### Aggregator
GraphSAGEの肝となるaggregatorを設定するにあたって, ノードを任意の順に扱うことができるようにシンメトリックなaggregatorを選択する必要があります.
そこで次のaggregatorが提案されました.

- Mean Aggregator
- LSTM Aggregator
- Pooling Aggregator

#### Mean Aggregator
その名の通りサンプリングした周辺ノードの平均を取るだけのシンプルなaggregatorです.

これはKipfらによって提案されているtransductive GCNで用いられているconvolutionの操作とほぼ等価であるとされています.

GCNをinductiveにするためにはaggregatorとconcatの操作を次の式で置き換えることで実現します.

$h^{k}_{v} \gets \sigma(W \cdot MEAN(\{h^{k-1}_{v}\} \cup \{h^{k-1}_{u}, \forall u \in \mathcal{N}(v)\}))$

なお, concatの操作はskip connectionと捉えることができるようです.

#### LSTM Aggregator
表現力が高いが, その構造上ノードの順不同性に対応できないaggregatorです.
したがってノード間に順序の存在するようなグラフに対して有効であると考えられます.

#### Pooling Aggregator
各ノードの特徴ベクトルをMLPにより変換し, それらの中で最大となるものを選択するaggregatorです.

$AGGREGATE_{k} = max(\{\sigma (W_{pool}h^{k}_{u_{i}} + b), \forall u_{i} \in \mathcal(N)(v)\})$

の式で表されます.
MLPは各ノードの特徴ベクトルを抽出すると考えることができ,
max-poolingを行うことで効率よく近傍ノードの特徴を捉えることができるとされます.

### Loss Function
loss functionは次のように定義します.

$J_{\mathcal{G}}(z_{u}) = - \log(\sigma(z_{u}^{\top}z_{v})) - Q \cdot \mathbb{E}_{v_{n} \sim P_{n}(v)}\log(\sigma(-z_{u}^{\top}z_{v_{n}}))$

これはWord2Vecのnegative samplingを用いた高速化を元にしたloss functionであり, 次の論文に詳しく述べられています.
[Distributed Representations of Words and Phrases and their Compositionality
](https://arxiv.org/abs/1310.4546) [Tomas+]

このloss functionを導入することで似ているノードは似た表現, 関係性の低いノードは異なる表現となることが期待されます.

## データセット
GraphSAGEの性能を見るために次の3つのデータセットを用いています.

- WoS Citation dataset
  Web of Science(トムソン・ロイターによる学術データベース)に収録された論文のカテゴリ分類.
- Reddit dataset
  Redditに投稿されたポストがどのコミュニティに属するかを予測.
- PPI dataset
  ヒトの各臓器に存在するタンパク質についてGene Ontology(GO)に基づいた機能を予測.

上記2つは既存のグラフに新規ノードを追加しそのノードを分類するタスクであり, PPIデータセットを用いたタスクは複数のグラフ(臓器1つにつき1つのグラフ)を学習し, 新たなグラフで分類を行うものです.

## 結果
### 既存手法との比較
![graphsage result1](https://raw.githubusercontent.com/natsukium/blog-post/master/images/2019-02-13/graphsage_result1.png)

単なるロジスティック回帰(Raw features)やMatrix FactorizationをベースとしたDeepWalkと比較して圧倒的なF1 scoreを記録しています.
LSTM aggregatorやPooling aggregatorを用いたネットワークは比較対象のGCNよりも良いという結果となりました.
今回のデータセットはグラフのノードが順不同ですが, LSTM aggregatorはその表現力の高さから良いスコアを出したと考えられます.

### パラメータ探索など
![graphsage result2](https://raw.githubusercontent.com/natsukium/blog-post/master/images/2019-02-13/graphsage_result2.png)

さらにGraphSAGEは推論が速いことも示されました.(Fig.2A)

イテレーション数$K$に関しては, $K=2$がパフォーマンス, コストのバランスがよく, このとき$S_{1} = S_{2}$($S_{i}$はイテレーション$i$のときのサンプル数)としたとき, $S_{i} \approx 20$あたりでF1 scoreはある程度大きくなり, 学習コストの増加率が大きくなることがわかりました.(Fig.2B)

## まとめ
GraphSAGEはaggregatorを学習することによって未知のノードを効果的に分類することができるようになりました.
モデルのパフォーマンス上昇と学習コスト低減を近傍ノードのサンプリングを行うことによって効果的に達成しました.
